-- if true then return {} end -- 暫時停用
return {
  {
    -- 呼叫AI聊天視窗用
    "yetone/avante.nvim",

    -- Key Bindings
    -- Leaderaa 	show sidebar
    -- Leaderat 	toggle sidebar visibility
    -- Leaderar 	refresh sidebar
    -- Leaderaf 	switch sidebar focus
    -- Leadera? 	select model
    -- Leaderae 	edit selected blocks
    -- LeaderaS 	stop current AI request
    -- Leaderah 	select between chat histories
    -- co 	choose ours
    -- ct 	choose theirs
    -- ca 	choose all theirs
    -- c0 	choose none
    -- cb 	choose both
    -- cc 	choose cursor
    -- ]x 	move to previous conflict
    -- [x 	move to next conflict
    -- [[ 	jump to previous codeblocks (results window)
    -- ]] 	jump to next codeblocks (results windows)

    event = "VeryLazy",
    version = false, -- Never set this value to "*"! Never!
    keys = {
      { "<C-A-i>", "<cmd>AvanteToggle<cr>", desc = "Avante Chat Toggle" },
    },
    opts = {
      providers = {
        ollama = {
          {{ if .ollamaKey -}}
          api_key_name = "OLLAMA_API_KEY",
          {{ end -}}
          endpoint = {{ .ollamaUrl | quote }},
          model = "qwen2.5-coder:7b",
          disable_tools = true, -- Open-source models often do not support tools.

          -- Fix: Always show `ollama: API key not set, continuing without authentication`
          parse_api_key = function()
            return vim.env.OLLAMA_API_KEY
          end,
        },
      },

      provider = "ollama",
      behaviour = {
        auto_suggestions = false, -- Experimental stage
        auto_set_highlight_group = true,
        auto_set_keymaps = true,
        auto_apply_diff_after_generation = false,
        support_paste_from_clipboard = false,
        minimize_diff = true, -- Whether to remove unchanged lines when applying a code block
        enable_token_counting = true, -- Whether to enable token counting. Default to true.
      },
      mappings = {
        --- @class AvanteConflictMappings
        suggestion = {
          accept = "<C-l>",
          next = "<C-]>",
          prev = "<C-[>",
          dismiss = "<C-'>",
        },
      },
    },
    -- if you want to build from source then do `make BUILD_FROM_SOURCE=true`
    build = "make",
    -- build = "powershell -ExecutionPolicy Bypass -File Build.ps1 -BuildFromSource false" -- for windows
    dependencies = {
      "nvim-treesitter/nvim-treesitter",
      "stevearc/dressing.nvim",
      "nvim-lua/plenary.nvim",
      "MunifTanjim/nui.nvim",
      --- The below dependencies are optional,
      "echasnovski/mini.pick", -- for file_selector provider mini.pick
      "nvim-telescope/telescope.nvim", -- for file_selector provider telescope
      "hrsh7th/nvim-cmp", -- autocompletion for avante commands and mentions
      "ibhagwan/fzf-lua", -- for file_selector provider fzf
      "nvim-tree/nvim-web-devicons", -- or echasnovski/mini.icons
      "zbirenbaum/copilot.lua", -- for providers='copilot'
      {
        -- support for image pasting
        "HakonHarnes/img-clip.nvim",
        event = "VeryLazy",
        opts = {
          -- recommended settings
          default = {
            embed_image_as_base64 = false,
            prompt_for_file_name = false,
            drag_and_drop = {
              insert_mode = true,
            },
            -- required for Windows users
            use_absolute_path = true,
          },
        },
      },
      {
        -- Make sure to set this up properly if you have lazy=true
        'MeanderingProgrammer/render-markdown.nvim',
        opts = {
          file_types = { "markdown", "Avante" },
        },
        ft = { "markdown", "Avante" },
      },
    },
    config = function(_, opts)
      -- 這裡動態設定 Neovim 的環境變數（不會傳給 shell）
      {{ if .ollamaKey }}vim.env.OLLAMA_API_KEY = {{ .ollamaKey | quote }}{{ end }}
      require("avante").setup(opts)

      -- -- 配置 Copilot
      -- require("copilot").setup()
    end,
  },

  {
    -- 自動補全用
    'huggingface/llm.nvim',
    dependencies = { 'kevinhwang91/nvim-ufo' }, -- 確保 ufo 先載入
    event = 'VeryLazy', -- 啟動後自動載入
    opts = {
      backend = "openai",
      -- chat_mode = true,
      -- model = "qwen2.5-coder:1.5b", -- 實測發現不相容，先關掉
      -- model = "codellama:7b",
      -- model = "starcoder2:3b",
      -- model = "starcoder:1b",
      model = "stable-code:3b",
      url = {{ .ollamaUrl | quote }}, -- llm-ls uses "/api/generate"
      {{ if .ollamaKey }}api_token = {{ .ollamaKey | quote }},{{ end }}
      -- -- cf https://github.com/ollama/ollama/blob/main/docs/api.md#parametersu can set any field:value pair here it will be passed as is to the backend
      request_body = {
        -- Modelfile options for the model you use
        options = {
          temperature = 0.2,
          top_p = 0.95,
        }
      },
      -- on_response = function(output)
      --   return output:gsub("^```[%w]*\n?", ""):gsub("```$", "")
      -- end,
    },
  },
}
